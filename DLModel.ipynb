{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load UCI Adult Dataset\n",
        "column_names = [\n",
        "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
        "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
        "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
        "]\n",
        "url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data'\n",
        "data = pd.read_csv(url, header=None, names=column_names, na_values=' ?', skipinitialspace=True)\n",
        "data.dropna(inplace=True)\n",
        "\n",
        "# Encode income\n",
        "data['income'] = data['income'].apply(lambda x: 1 if x == '>50K' else 0)\n",
        "\n",
        "# Encode categorical features\n",
        "categorical_cols = data.select_dtypes(include='object').columns\n",
        "label_encoders = {col: LabelEncoder().fit(data[col]) for col in categorical_cols}\n",
        "for col, le in label_encoders.items():\n",
        "    data[col] = le.transform(data[col])\n",
        "\n",
        "# Sensitive attribute: sex (for fairness)\n",
        "sensitive_feature = data['sex'].values\n",
        "\n",
        "# Prepare features and labels\n",
        "X = data.drop('income', axis=1)\n",
        "y = data['income']\n",
        "\n",
        "# Train/Test Split\n",
        "X_train, X_test, y_train, y_test, sensitive_train, sensitive_test = train_test_split(\n",
        "    X, y, sensitive_feature, test_size=0.2, random_state=42)\n",
        "\n",
        "# Scale Features\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Convert to Tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)\n",
        "\n",
        "# Define Task and Audit Networks\n",
        "class TaskNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(TaskNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 2)\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))\n",
        "\n",
        "class AuditNet(nn.Module):\n",
        "    def __init__(self, input_dim=2):\n",
        "        super(AuditNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "    def forward(self, x):\n",
        "        return torch.sigmoid(self.fc2(F.relu(self.fc1(x))))\n",
        "\n",
        "class SANNet(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(SANNet, self).__init__()\n",
        "        self.task_net = TaskNet(input_dim)\n",
        "        self.audit_net = AuditNet()\n",
        "    def forward(self, x):\n",
        "        task_out = self.task_net(x)\n",
        "        audit_out = self.audit_net(task_out)\n",
        "        return task_out, audit_out\n",
        "\n",
        "# Model Training\n",
        "model = SANNet(X_train.shape[1])\n",
        "task_opt = optim.Adam(model.task_net.parameters(), lr=0.001)\n",
        "audit_opt = optim.Adam(model.audit_net.parameters(), lr=0.001)\n",
        "task_loss_fn = nn.CrossEntropyLoss()\n",
        "audit_loss_fn = nn.BCELoss()\n",
        "\n",
        "for epoch in range(10):\n",
        "    model.train()\n",
        "    task_out, audit_out = model(X_train_tensor)\n",
        "    task_loss = task_loss_fn(task_out, y_train_tensor)\n",
        "    audit_loss = audit_loss_fn(audit_out.squeeze(), y_train_tensor.float())\n",
        "    total_loss = task_loss + audit_loss\n",
        "    task_opt.zero_grad()\n",
        "    audit_opt.zero_grad()\n",
        "    total_loss.backward()\n",
        "    task_opt.step()\n",
        "    audit_opt.step()\n",
        "    print(f\"Epoch {epoch+1} - Loss: {total_loss.item():.4f}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    task_preds = model.task_net(X_test_tensor).argmax(dim=1)\n",
        "    audit_scores = model.audit_net(model.task_net(X_test_tensor)).squeeze()\n",
        "    audit_preds = (audit_scores > 0.5).float()\n",
        "\n",
        "accuracy = accuracy_score(y_test_tensor, task_preds)\n",
        "audit_accuracy = (audit_preds == y_test_tensor.float()).float().mean().item()\n",
        "\n",
        "print(f\"\\n✅ TaskNet Accuracy: {accuracy:.4f}\")\n",
        "print(f\"✅ AuditNet Ethical Compliance Accuracy: {audit_accuracy:.4f}\")\n",
        "\n",
        "# Fairness Metrics\n",
        "def check_demographic_parity(preds, sensitive):\n",
        "    for group in np.unique(sensitive):\n",
        "        rate = np.mean(preds[sensitive == group] == 1)\n",
        "        print(f\"Demographic Parity (Group {group}): {rate:.4f}\")\n",
        "\n",
        "def check_equalized_odds(preds, labels, sensitive):\n",
        "    for group in np.unique(sensitive):\n",
        "        group_preds = preds[sensitive == group]\n",
        "        group_labels = labels[sensitive == group]\n",
        "        cm = confusion_matrix(group_labels, group_preds)\n",
        "        if cm.shape == (2, 2):\n",
        "            TN, FP, FN, TP = cm.ravel()\n",
        "            FPR = FP / (FP + TN) if (FP + TN) else 0\n",
        "            TPR = TP / (TP + FN) if (TP + FN) else 0\n",
        "            print(f\"Group {group} - FPR: {FPR:.4f}, TPR: {TPR:.4f}\")\n",
        "        else:\n",
        "            print(f\"Group {group} - Not enough data for confusion matrix.\")\n",
        "\n",
        "check_demographic_parity(task_preds.numpy(), sensitive_test)\n",
        "check_equalized_odds(task_preds.numpy(), y_test_tensor.numpy(), sensitive_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vaFOXKlqFjfg",
        "outputId": "31cc348b-96ec-400c-af87-b557a20c62d3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Loss: 1.2833\n",
            "Epoch 2 - Loss: 1.2718\n",
            "Epoch 3 - Loss: 1.2607\n",
            "Epoch 4 - Loss: 1.2500\n",
            "Epoch 5 - Loss: 1.2397\n",
            "Epoch 6 - Loss: 1.2297\n",
            "Epoch 7 - Loss: 1.2201\n",
            "Epoch 8 - Loss: 1.2108\n",
            "Epoch 9 - Loss: 1.2019\n",
            "Epoch 10 - Loss: 1.1932\n",
            "\n",
            "✅ TaskNet Accuracy: 0.7827\n",
            "✅ AuditNet Ethical Compliance Accuracy: 0.7588\n",
            "Demographic Parity (Group 0): 0.0428\n",
            "Demographic Parity (Group 1): 0.1110\n",
            "Group 0 - FPR: 0.0354, TPR: 0.1030\n",
            "Group 1 - FPR: 0.0472, TPR: 0.2564\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome to Colab",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}